{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RLLAB1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNiCqZTAa7aLt8zRerWoeh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shibu778/MyNotebooks/blob/master/RLLAB1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKDUwHkwB3q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "### Interface\n",
        "class Environment(object):\n",
        "\n",
        "    def reset(self):\n",
        "        raise NotImplementedError('Inheriting classes must override reset.')\n",
        "\n",
        "    def actions(self):\n",
        "        raise NotImplementedError('Inheriting classes must override actions.')\n",
        "\n",
        "    def step(self):\n",
        "        raise NotImplementedError('Inheriting classes must override step')\n",
        "\n",
        "class ActionSpace(object):\n",
        "    \n",
        "    def __init__(self, actions):\n",
        "        self.actions = actions\n",
        "        self.n = len(actions)\n",
        "        \n",
        "### SimpleRoomsEnv Environment \n",
        "\n",
        "class SimpleRoomsEnv(Environment):\n",
        "    \"\"\"Define a simple 4-room environment\"\"\"\n",
        "    \"\"\"actions: 0 - north, 1 - east, 2 - west, 3 - south\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SimpleRoomsEnv, self).__init__()\n",
        "\n",
        "        # define state and action space\n",
        "        self.S = range(16)\n",
        "        self.action_space = ActionSpace(range(4))\n",
        "\n",
        "        # define reward structure\n",
        "        self.R = [0] * len(self.S)\n",
        "        self.R[15] = 1\n",
        "        \n",
        "        # define transitions\n",
        "        self.P = {}\n",
        "        self.P[0] = [1, 4]\n",
        "        self.P[1] = [0, 2, 5]\n",
        "        self.P[2] = [1, 3, 6]\n",
        "        self.P[3] = [2, 7]\n",
        "        self.P[4] = [0, 5, 8]\n",
        "        self.P[5] = [1, 4]\n",
        "        self.P[6] = [2, 7]\n",
        "        self.P[7] = [3, 6, 11]\n",
        "        self.P[8] = [4, 9, 12]\n",
        "        self.P[9] = [8, 13]\n",
        "        self.P[10] = [11, 14]\n",
        "        self.P[11] = [7, 10, 15]\n",
        "        self.P[12] = [8, 13]\n",
        "        self.P[13] = [9, 12, 14]\n",
        "        self.P[14] = [10, 13, 15]\n",
        "        self.P[15] = [11, 14]\n",
        "\n",
        "        self.max_trajectory_length = 50\n",
        "        self.tolerance = 0.1\n",
        "        self._rendered_maze = self._render_maze()\n",
        "        self.s = 0\n",
        "        self.nstep = 0\n",
        "        \n",
        "    def step(self, action):\n",
        "        s_prev = self.s\n",
        "        self.s = self.single_step(self.s, action)\n",
        "        reward = self.single_reward(self.s, s_prev, self.R)\n",
        "        self.nstep += 1\n",
        "        self.is_reset = False\n",
        "\n",
        "        if (reward < -1. * (self.tolerance) or reward > self.tolerance) or self.nstep == self.max_trajectory_length:\n",
        "            self.reset()\n",
        "\n",
        "        return (self._convert_state(self.s), reward, self.is_reset, '')\n",
        "    \n",
        "    def single_step(self, s, a):\n",
        "        if a < 0 or a > 3:\n",
        "            raise ValueError('Unknown action', a)\n",
        "        if a == 0 and (s-4 in self.P[s]):\n",
        "            s -= 4\n",
        "        elif a == 1 and (s+1 in self.P[s]):\n",
        "            s += 1\n",
        "        elif a == 2 and (s-1 in self.P[s]):\n",
        "            s -= 1\n",
        "        elif a == 3 and (s+4 in self.P[s]):\n",
        "            s += 4\n",
        "        return s\n",
        "\n",
        "    def single_reward(self, s, s_prev, rewards):\n",
        "        if s == s_prev:\n",
        "            return 0\n",
        "        return rewards[s]\n",
        "    \n",
        "    def reset(self):\n",
        "        self.nstep = 0\n",
        "        self.s = 0\n",
        "        self.is_reset = True\n",
        "        return self._convert_state(self.s)\n",
        "    \n",
        "    def _convert_state(self, s):\n",
        "        converted = np.zeros(len(self.S), dtype=np.float32)\n",
        "        converted[s] = 1\n",
        "        return converted\n",
        "    \n",
        "    def _get_render_coords(self, s):\n",
        "        return (int(s / 4) * 4, (s % 4) * 4)\n",
        "    \n",
        "    def _render_maze(self):\n",
        "        # draw background and grid lines\n",
        "        maze = np.zeros((17, 17))\n",
        "        for x in range(0, 17, 4):\n",
        "            maze[x, :] = 0.5\n",
        "        for y in range(0, 17, 4):\n",
        "            maze[:, y] = 0.5\n",
        "\n",
        "        # draw reward and transitions\n",
        "        for s in range(16):\n",
        "            if self.R[s] != 0:\n",
        "                x, y = self._get_render_coords(s)\n",
        "                maze[x+1:x+4, y+1:y+4] = self.R[s]\n",
        "            if self.single_step(s, 0) == s:\n",
        "                x, y = self._get_render_coords(s)\n",
        "                maze[x, y:y+5] = -1\n",
        "            if self.single_step(s, 1) == s:\n",
        "                x, y = self._get_render_coords(s)\n",
        "                maze[x:x+5, y+4] = -1\n",
        "            if self.single_step(s, 2) == s:\n",
        "                x, y = self._get_render_coords(s)\n",
        "                maze[x:x+5, y] = -1\n",
        "            if self.single_step(s, 3) == s:\n",
        "                x, y = self._get_render_coords(s)\n",
        "                maze[x+4, y:y+4] = -1\n",
        "        return maze\n",
        "\n",
        "    def render(self, mode = 'rgb_array'):\n",
        "        assert mode == 'rgb_array', 'Unknown mode: %s' % mode\n",
        "        img = np.array(self._rendered_maze, copy=True)\n",
        "\n",
        "        # draw current agent location\n",
        "        x, y = self._get_render_coords(self.s)\n",
        "        img[x+1:x+4, y+1:y+4] = 2.0\n",
        "        return img\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7AXX2UWPJnb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "e00a2d3d-3527-4e72-aab1-6cde0fb1b22c"
      },
      "source": [
        "import random\n",
        "sr = SimpleRoomsEnv()\n",
        "for i in range(100):\n",
        "  a = random.choice([0,1,2,3])\n",
        "  sr.step(a)\n",
        "#[ab,bb,cc,dd]=sr.step(1)\n",
        "img = sr.render(mode = 'rgb_array')\n",
        "\n",
        "img"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1. , -1. , -1. , -1. , -1. , -1. , -1. , -1. , -1. , -1. , -1. ,\n",
              "        -1. , -1. , -1. , -1. , -1. , -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  2. ,  2. ,  2. ,  0.5,  0. ,  0. ,\n",
              "         0. ,  0.5,  0. ,  0. ,  0. , -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  2. ,  2. ,  2. ,  0.5,  0. ,  0. ,\n",
              "         0. ,  0.5,  0. ,  0. ,  0. , -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  2. ,  2. ,  2. ,  0.5,  0. ,  0. ,\n",
              "         0. ,  0.5,  0. ,  0. ,  0. , -1. ],\n",
              "       [-1. ,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5, -1. ,  0.5,  0.5,\n",
              "         0.5,  0.5,  0.5,  0.5,  0.5, -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. , -1. ,  0. ,  0. ,\n",
              "         0. ,  0.5,  0. ,  0. ,  0. , -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. , -1. ,  0. ,  0. ,\n",
              "         0. ,  0.5,  0. ,  0. ,  0. , -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. , -1. ,  0. ,  0. ,\n",
              "         0. ,  0.5,  0. ,  0. ,  0. , -1. ],\n",
              "       [-1. ,  0.5,  0.5,  0.5, -1. , -1. , -1. , -1. , -1. , -1. , -1. ,\n",
              "        -1. , -1. ,  0.5,  0.5,  0.5, -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. , -1. ,  0. ,  0. ,\n",
              "         0. ,  0.5,  0. ,  0. ,  0. , -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. , -1. ,  0. ,  0. ,\n",
              "         0. ,  0.5,  0. ,  0. ,  0. , -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. , -1. ,  0. ,  0. ,\n",
              "         0. ,  0.5,  0. ,  0. ,  0. , -1. ],\n",
              "       [-1. ,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5, -1. ,  0.5,  0.5,\n",
              "         0.5,  0.5,  0.5,  0.5,  0.5, -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,\n",
              "         0. ,  0.5,  1. ,  1. ,  1. , -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,\n",
              "         0. ,  0.5,  1. ,  1. ,  1. , -1. ],\n",
              "       [-1. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,  0. ,  0.5,  0. ,  0. ,\n",
              "         0. ,  0.5,  1. ,  1. ,  1. , -1. ],\n",
              "       [-1. , -1. , -1. , -1. , -1. , -1. , -1. , -1. , -1. , -1. , -1. ,\n",
              "        -1. , -1. , -1. , -1. , -1. , -1. ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaY-XILHU34o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "016c33dd-6e2f-4560-c69c-9ea79daefa4b"
      },
      "source": [
        "import matplotlib.pyplot as plt  \n",
        "  \n",
        "# creating a object  \n",
        "#im = Image.open(r\"C:\\Users\\System-Pc\\Desktop\\tree.jpg\")  \n",
        "  \n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7381a26780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh5PRppPtYLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install StringIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL0cDWMeU30P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from gym.envs.toy_text import discrete\n",
        "\n",
        "### CliffWalkingEnv Environment \n",
        "    \n",
        "class CliffWalkingEnv(discrete.DiscreteEnv):\n",
        "\n",
        "    metadata = {'render.modes': ['human', 'ansi', 'rgb_array']}\n",
        "    \n",
        "    def _limit_coordinates(self, coord):\n",
        "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
        "        coord[0] = max(coord[0], 0)\n",
        "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
        "        coord[1] = max(coord[1], 0)\n",
        "        return coord\n",
        "\n",
        "    def _calculate_transition_prob(self, current, delta):\n",
        "        new_position = np.array(current) + np.array(delta)\n",
        "        new_position = self._limit_coordinates(new_position).astype(int)\n",
        "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
        "        reward = -100.0 if self._cliff[tuple(new_position)] else -1.0\n",
        "        is_done = self._cliff[tuple(new_position)] or (tuple(new_position) == (3,11))\n",
        "        return [(1.0, new_state, reward, is_done)]\n",
        "\n",
        "    def __init__(self):\n",
        "        self.shape = (4, 12)\n",
        "\n",
        "        nS = np.prod(self.shape)\n",
        "        nA = 4\n",
        "\n",
        "        # Cliff Location\n",
        "        self._cliff = np.zeros(self.shape, dtype=np.bool)\n",
        "        self._cliff[3, 1:-1] = True\n",
        "\n",
        "        # Calculate transition probabilities\n",
        "        P = {}\n",
        "        for s in range(nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            P[s] = { a : [] for a in range(nA) }\n",
        "            #UP = 0\n",
        "            #RIGHT = 1\n",
        "            #DOWN = 2\n",
        "            #LEFT = 3\n",
        "            P[s][0] = self._calculate_transition_prob(position, [-1, 0])\n",
        "            P[s][1] = self._calculate_transition_prob(position, [0, 1])\n",
        "            P[s][2] = self._calculate_transition_prob(position, [1, 0])\n",
        "            P[s][3] = self._calculate_transition_prob(position, [0, -1])\n",
        "\n",
        "        # We always start in state (3, 0)\n",
        "        isd = np.zeros(nS)\n",
        "        isd[np.ravel_multi_index((3,0), self.shape)] = 1.0\n",
        "\n",
        "        super(CliffWalkingEnv, self).__init__(nS, nA, P, isd)\n",
        "\n",
        "    def _convert_state(self, state):\n",
        "        converted = np.unravel_index(state, self.shape)\n",
        "        return np.asarray(list(converted), dtype=np.float32)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.s = np.argmax(self.isd)\n",
        "        return self._convert_state(self.s)\n",
        "    \n",
        "    def step(self, action):\n",
        "        reward = self.P[self.s][action][0][2]\n",
        "        done = self.P[self.s][action][0][3]\n",
        "        info = {'prob':self.P[self.s][action][0][0]}\n",
        "        self.s = self.P[self.s][action][0][1]\n",
        "        return (self._convert_state(self.s), reward, done, info)\n",
        "    \n",
        "    def render(self, mode='rgb_array', close=False):\n",
        "        if close:\n",
        "            return\n",
        "\n",
        "        if mode == 'rgb_array':\n",
        "            maze = np.zeros((4, 12))\n",
        "            maze[self._cliff] = -1\n",
        "            maze[np.unravel_index(self.s, self.shape)] = 2.0\n",
        "            maze[(3,11)] = 0.5\n",
        "            img = np.array(maze, copy=True)\n",
        "            return img\n",
        "        \n",
        "        else:\n",
        "            outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "\n",
        "            for s in range(self.nS):\n",
        "                position = np.unravel_index(s, self.shape)\n",
        "\n",
        "                if self.s == s:\n",
        "                    output = \" x \"\n",
        "                elif position == (3,11):\n",
        "                    output = \" T \"\n",
        "                elif self._cliff[position]:\n",
        "                    output = \" C \"\n",
        "                else:\n",
        "                    output = \" o \"\n",
        "\n",
        "                if position[1] == 0:\n",
        "                    output = output.lstrip() \n",
        "                if position[1] == self.shape[1] - 1:\n",
        "                    output = output.rstrip() \n",
        "                    output += \"\\n\"\n",
        "\n",
        "                outfile.write(output)\n",
        "            outfile.write(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LzPbZNBsQgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "efc083c2-8bc7-4496-cc31-669e6a56e915"
      },
      "source": [
        "cw = CliffWalkingEnv()\n",
        "cw.step(1)\n",
        "cw.step(1)\n",
        "cw.step(0)\n",
        "img = cw.render()\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7377f2b668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW1oVwxWxJI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import lib.plotting as plotting\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import pylab\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "class Experiment(object):\n",
        "    def __init__(self, env, agent):\n",
        "        \n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        \n",
        "        self.episode_length = np.array([0])\n",
        "        self.episode_reward = np.array([0])\n",
        "        \n",
        "        self.fig = pylab.figure(figsize=(10, 5))\n",
        "        gs = gridspec.GridSpec(2, 2)\n",
        "        self.ax = pylab.subplot(gs[:, 0])\n",
        "        self.ax.xaxis.set_visible(False)\n",
        "        self.ax.yaxis.set_visible(False)\n",
        "        \n",
        "        if hasattr(self.env, '_cliff'): # Hardcode to nicely display grid for cliffwalkingenv\n",
        "            self.ax.xaxis.set_visible(True)\n",
        "            self.ax.yaxis.set_visible(True)\n",
        "            self.ax.set_xticks(np.arange(-.5, 12, 1), minor=True);\n",
        "            self.ax.set_yticks(np.arange(-.5, 4, 1), minor=True);\n",
        "            self.ax.grid(which='minor', color='w', linestyle='-', linewidth=1)\n",
        "            \n",
        "        if hasattr(self.env, 'winds'): # Hardcode to nicely display grid for windygridworldenv\n",
        "            self.ax.xaxis.set_visible(True)\n",
        "            self.ax.yaxis.set_visible(True)\n",
        "            self.ax.set_xticks(np.arange(-.5, 10, 1), minor=True);\n",
        "            self.ax.set_yticks(np.arange(-.5, 7, 1), minor=True);\n",
        "            self.ax.grid(which='minor', color='w', linestyle='-', linewidth=1)\n",
        "        \n",
        "        self.ax1 = pylab.subplot(gs[0, 1])\n",
        "        self.ax1.yaxis.set_label_position(\"right\")\n",
        "        self.ax1.set_ylabel('Length')\n",
        "        \n",
        "        self.ax1.set_xlim(0, max(10, len(self.episode_length)+1))\n",
        "        self.ax1.set_ylim(0, 51)\n",
        "        \n",
        "        self.ax2 = pylab.subplot(gs[1, 1])\n",
        "        self.ax2.set_xlabel('Episode')\n",
        "        self.ax2.yaxis.set_label_position(\"right\")\n",
        "        self.ax2.set_ylabel('Reward')\n",
        "        self.ax2.set_xlim(0, max(10, len(self.episode_reward)+1))\n",
        "        self.ax2.set_ylim(0, 2)\n",
        "        \n",
        "        self.line, = self.ax1.plot(range(len(self.episode_length)),self.episode_length)\n",
        "        self.line2, = self.ax2.plot(range(len(self.episode_reward)),self.episode_reward)\n",
        "        \n",
        "    def update_display_step(self):\n",
        "        if not hasattr(self, 'imgplot'):\n",
        "            self.imgplot = self.ax.imshow(self.env.render(mode='rgb_array'), interpolation='none', cmap='viridis')\n",
        "        else:\n",
        "            self.imgplot.set_data(self.env.render(mode='rgb_array'))\n",
        "    \n",
        "        self.fig.canvas.draw()\n",
        "        \n",
        "    def update_display_episode(self):  \n",
        "        self.line.set_data(range(len(self.episode_length)),self.episode_length)\n",
        "        self.ax1.set_xlim(0, max(10, len(self.episode_length)+1))\n",
        "        self.ax1.set_ylim(0, max(self.episode_length)+1)\n",
        "        \n",
        "        self.line2.set_data(range(len(self.episode_reward)),self.episode_reward)\n",
        "        self.ax2.set_xlim(0, max(10, len(self.episode_reward)+1))\n",
        "        self.ax2.set_ylim(min(self.episode_reward)-1, max(self.episode_reward)+1)\n",
        "        \n",
        "        self.fig.canvas.draw()     \n",
        "        \n",
        "    def run_bandit(self, max_number_of_trials=1000, display_frequency=1):\n",
        "        self.fig.clf()\n",
        "        \n",
        "        print(\"Distribution:\", self.env.distribution, self.env.reward_parameters, flush = True)\n",
        "        print(\"Optimal arm:\", self.env.optimal_arm, flush = True)\n",
        "        \n",
        "        if self.env.distribution != \"normal\":\n",
        "            plotting.plot_arm_rewards(self.env.reward_parameters)\n",
        "        #else:\n",
        "            #plotting.plot_arm_rewards(self.env.reward_parameters[0])\n",
        "        \n",
        "        stats = plotting.TimestepStats(\n",
        "            cumulative_rewards=np.zeros(max_number_of_trials),\n",
        "            regrets=np.zeros(max_number_of_trials))   \n",
        "            \n",
        "        cumulative_reward = 0.0\n",
        "        cumulative_regret = 0.0\n",
        "        \n",
        "        for trial in range(max_number_of_trials):\n",
        "            action = self.agent.act()\n",
        "            \n",
        "            _ , reward, done, _ = self.env.step(action)       \n",
        "            self.agent.feedback(action, reward)\n",
        "            cumulative_reward += reward\n",
        "\n",
        "            gap = self.env.compute_gap(action)\n",
        "            if action != self.env.optimal_arm:\n",
        "                cumulative_regret += gap\n",
        "\n",
        "            stats.cumulative_rewards[trial] = cumulative_reward\n",
        "            stats.regrets[trial] = cumulative_regret\n",
        "\n",
        "        print(\"--------------------------------------------------\", flush = True)\n",
        "        print(\"Policy:\", self.agent.name, \"\\nAverage Reward:\", cumulative_reward / max_number_of_trials, \\\n",
        "                \"\\nAverage Regret:\", cumulative_regret / max_number_of_trials, flush = True)\n",
        "        print(\"Arm pulls:\", self.agent.total_counts, flush = True)\n",
        "         \n",
        "        plotting.plot_reward_regret(stats)\n",
        "        \n",
        "    def run_agent(self, max_number_of_episodes=100, interactive = False, display_frequency=1):\n",
        "\n",
        "        # repeat for each episode\n",
        "        for episode_number in range(max_number_of_episodes):\n",
        "            \n",
        "            # initialize state\n",
        "            state = self.env.reset()\n",
        "            \n",
        "            done = False # used to indicate terminal state\n",
        "            R = 0 # used to display accumulated rewards for an episode\n",
        "            t = 0 # used to display accumulated steps for an episode i.e episode length\n",
        "            \n",
        "            # repeat for each step of episode, until state is terminal\n",
        "            while not done:\n",
        "                \n",
        "                # increase step counter - for display\n",
        "                t += 1\n",
        "                \n",
        "                # choose action from state \n",
        "                action = self.agent.act(state)\n",
        "                \n",
        "                # take action, observe reward and next state\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                \n",
        "                # state <- next state\n",
        "                state = next_state\n",
        "                \n",
        "                R += reward # accumulate reward - for display\n",
        "                \n",
        "                # if interactive display, show update for each step\n",
        "                if interactive:\n",
        "                    self.update_display_step()\n",
        "            \n",
        "            self.episode_length = np.append(self.episode_length,t) # keep episode length - for display\n",
        "            self.episode_reward = np.append(self.episode_reward,R) # keep episode reward - for display \n",
        "            \n",
        "            # if interactive display, show update for the episode\n",
        "            if interactive:\n",
        "                self.update_display_episode()\n",
        "        \n",
        "        # if not interactive display, show graph at the end\n",
        "        if not interactive:\n",
        "            self.fig.clf()\n",
        "            stats = plotting.EpisodeStats(\n",
        "                episode_lengths=self.episode_length,\n",
        "                episode_rewards=self.episode_reward,\n",
        "                episode_running_variance=np.zeros(max_number_of_episodes))\n",
        "            plotting.plot_episode_stats(stats, display_frequency)\n",
        "        \n",
        "  \n",
        "    def run_qlearning(self, max_number_of_episodes=100, interactive = False, display_frequency=1):\n",
        "\n",
        "        # repeat for each episode\n",
        "        for episode_number in range(max_number_of_episodes):\n",
        "            \n",
        "            # initialize state\n",
        "            state = self.env.reset()\n",
        "            \n",
        "            done = False # used to indicate terminal state\n",
        "            R = 0 # used to display accumulated rewards for an episode\n",
        "            t = 0 # used to display accumulated steps for an episode i.e episode length\n",
        "            \n",
        "            # repeat for each step of episode, until state is terminal\n",
        "            while not done:\n",
        "                \n",
        "                t += 1 # increase step counter - for display\n",
        "                \n",
        "                # choose action from state using policy derived from Q\n",
        "                action = self.agent.act(state)\n",
        "                \n",
        "                # take action, observe reward and next state\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                \n",
        "                # agent learn (Q-Learning update)\n",
        "                self.agent.learn(state, action, reward, next_state, done)\n",
        "                \n",
        "                # state <- next state\n",
        "                state = next_state\n",
        "                \n",
        "                R += reward # accumulate reward - for display\n",
        "                \n",
        "                # if interactive display, show update for each step\n",
        "                if interactive:\n",
        "                    self.update_display_step()\n",
        "            \n",
        "            self.episode_length = np.append(self.episode_length,t) # keep episode length - for display\n",
        "            self.episode_reward = np.append(self.episode_reward,R) # keep episode reward - for display \n",
        "            \n",
        "            # if interactive display, show update for the episode\n",
        "            if interactive:\n",
        "                self.update_display_episode()\n",
        "        \n",
        "        # if not interactive display, show graph at the end\n",
        "        if not interactive:\n",
        "            self.fig.clf()\n",
        "            stats = plotting.EpisodeStats(\n",
        "                episode_lengths=self.episode_length,\n",
        "                episode_rewards=self.episode_reward,\n",
        "                episode_running_variance=np.zeros(max_number_of_episodes))\n",
        "            plotting.plot_episode_stats(stats, display_frequency)\n",
        "            \n",
        "    def run_sarsa(self, max_number_of_episodes=100, interactive = False, display_frequency=1):\n",
        "\n",
        "        # repeat for each episode\n",
        "        for episode_number in range(max_number_of_episodes):\n",
        "            \n",
        "            # initialize state\n",
        "            state = self.env.reset()\n",
        "\n",
        "            done = False # used to indicate terminal state\n",
        "            R = 0 # used to display accumulated rewards for an episode\n",
        "            t = 0 # used to display accumulated steps for an episode i.e episode length\n",
        "            \n",
        "            # choose action from state using policy derived from Q\n",
        "            action = self.agent.act(state)\n",
        "            \n",
        "            # repeat for each step of episode, until state is terminal\n",
        "            while not done:\n",
        "                \n",
        "                t += 1 # increase step counter - for display\n",
        "                \n",
        "                # take action, observe reward and next state\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                \n",
        "                # choose next action from next state using policy derived from Q\n",
        "                next_action = self.agent.act(next_state)\n",
        "                \n",
        "                # agent learn (SARSA update)\n",
        "                self.agent.learn(state, action, reward, next_state, next_action)\n",
        "                \n",
        "                # state <- next state, action <- next_action\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "\n",
        "                R += reward # accumulate reward - for display\n",
        "                \n",
        "                # if interactive display, show update for each step\n",
        "                if interactive:\n",
        "                    self.update_display_step()\n",
        "            \n",
        "            self.episode_length = np.append(self.episode_length,t) # keep episode length - for display\n",
        "            self.episode_reward = np.append(self.episode_reward,R) # keep episode reward - for display \n",
        "            \n",
        "            # if interactive display, show update for the episode\n",
        "            if interactive:\n",
        "                self.update_display_episode()\n",
        "        \n",
        "        # if not interactive display, show graph at the end\n",
        "        if not interactive:\n",
        "            self.fig.clf()\n",
        "            stats = plotting.EpisodeStats(\n",
        "                episode_lengths=self.episode_length,\n",
        "                episode_rewards=self.episode_reward,\n",
        "                episode_running_variance=np.zeros(max_number_of_episodes))\n",
        "            plotting.plot_episode_stats(stats, display_frequency)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}